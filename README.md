# Jarvis AI - Pers√∂nlicher KI-Assistent

Ein intelligenter pers√∂nlicher Assistent basierend auf einer Microservices-Architektur mit LLM-Integration.

## √úbersicht

Jarvis ist ein modulares KI-System, das folgende Hauptfunktionen bietet:
- üìö Fakten-Verwaltung und semantische Dokumentensuche
- üó£Ô∏è Spracherkennung (ASR) und Sprachausgabe (TTS) auf Deutsch
- üìÑ Automatische Dokumentenverarbeitung (NAS, E-Mails)
- ü§ñ LLM-gest√ºtzte Konversation (Ollama)
- ‚è∞ Proaktive Erinnerungen und Benachrichtigungen

## Architektur (PR1: Basis-Infrastruktur)

Das System wird in mehreren Phasen implementiert. Diese erste Phase (PR1) umfasst die Basis-Infrastruktur:

1. **llama**: Ollama LLM Service (Port 11434)
2. **chroma**: ChromaDB Vector Database (Port 8001)
3. **toolserver**: Facts Database + Vector Search API (Port 8002)

Weitere Services folgen in sp√§teren PRs.

## Schnellstart

### Voraussetzungen

- Docker und Docker Compose
- Mind. 8GB RAM

### Installation

1. Repository klonen:
```bash
git clone https://github.com/b4llu97/Jarvis.git
cd Jarvis
```

2. Umgebungsvariablen konfigurieren:
```bash
# Die config/.env Datei ist bereits vorhanden
# Bei Bedarf anpassen (z.B. IMAP-Zugangsdaten)
```

3. Services starten:
```bash
docker-compose up -d
```

4. LLM-Modell herunterladen:
```bash
docker exec jarvis-llama ollama pull llama3.1
```

### Verwendung

#### Toolserver API

Facts speichern und abrufen:
```bash
# Fakt speichern
curl -X PUT http://localhost:8002/v1/facts/versicherung.gebaeude.summe \
  -H "Content-Type: application/json" \
  -d '{"value":"980000 CHF"}'

# Fakt abrufen
curl http://localhost:8002/v1/facts/versicherung.gebaeude.summe

# Alle Facts auflisten
curl http://localhost:8002/v1/facts
```

Dokument hinzuf√ºgen:
```bash
curl -X POST http://localhost:8002/v1/documents \
  -H "Content-Type: application/json" \
  -d '{"text":"Dies ist ein Testdokument √ºber Versicherungen", "metadata":{"source":"test"}}'
```

Semantische Suche:
```bash
curl -X POST http://localhost:8002/v1/search \
  -H "Content-Type: application/json" \
  -d '{"query":"Versicherung", "n_results":5}'
```

## Konfiguration

### config/.env

Hauptkonfigurationsdatei mit allen Umgebungsvariablen:
- LLM-Einstellungen (Ollama Host, Modell)
- Chroma DB Einstellungen
- Platzhalter f√ºr zuk√ºnftige Features (IMAP, Telegram)

### config/system_prompt.txt

System-Prompt f√ºr das LLM mit Anweisungen und Richtlinien (in Deutsch).

### config/persona_prompt.txt

Pers√∂nlichkeitsdefinition f√ºr Jarvis (Kommunikationsstil, Tonfall).

## Entwicklung

### Service-Struktur

```
services/toolserver/
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ app/
    ‚îú‚îÄ‚îÄ main.py
    ‚îú‚îÄ‚îÄ database.py
    ‚îú‚îÄ‚îÄ models.py
    ‚îî‚îÄ‚îÄ tools.py
```

### Logs anzeigen

```bash
# Alle Services
docker-compose logs -f

# Einzelner Service
docker-compose logs -f toolserver
```

### Services neu bauen

```bash
docker-compose up -d --build
```

## Tests

### Verf√ºgbare Test-Befehle

```bash
# Toolserver Health-Check
curl http://localhost:8002/health

# Chroma Health-Check
curl http://localhost:8001/api/v1/heartbeat

# Ollama Health-Check
curl http://localhost:11434/api/tags

# Fakt-Speicherung testen
curl -X PUT http://localhost:8002/v1/facts/test.key \
  -H "Content-Type: application/json" \
  -d '{"value":"test_value"}'

# Fakt abrufen
curl http://localhost:8002/v1/facts/test.key

# Dokument hinzuf√ºgen und durchsuchen
curl -X POST http://localhost:8002/v1/documents \
  -H "Content-Type: application/json" \
  -d '{"text":"Testdokument", "metadata":{"source":"test"}}'

curl -X POST http://localhost:8002/v1/search \
  -H "Content-Type: application/json" \
  -d '{"query":"Test", "n_results":5}'
```

### Verifizierte Funktionalit√§t (PR1)

‚úÖ **Llama Service**
- Ollama l√§uft auf Port 11434
- Modell llama3.1:latest (8B Parameter, 4.9 GB) erfolgreich geladen

‚úÖ **Chroma Service**  
- ChromaDB l√§uft auf Port 8001
- Vector Database ist erreichbar und funktional

‚úÖ **Toolserver Service**
- API l√§uft auf Port 8002
- Fakten-Speicherung (SQLite): ‚úÖ Funktioniert
- Fakten-Abruf: ‚úÖ Funktioniert  
- Dokument-Hinzuf√ºgen (Chroma): ‚úÖ Funktioniert
- Semantische Suche: ‚úÖ Funktioniert (Distanz: 0.72)
- Tool-Definitionen API: ‚úÖ Liefert 3 Tools (get_fact, set_fact, search_docs)

**Test-Beispiel:**
```bash
# Versicherungssumme speichern
curl -X PUT http://localhost:8002/v1/facts/versicherung.gebaeude.summe \
  -H "Content-Type: application/json" \
  -d '{"value":"980000 CHF"}'

# Dokument √ºber Geb√§udeversicherungen hinzuf√ºgen
curl -X POST http://localhost:8002/v1/documents \
  -H "Content-Type: application/json" \
  -d '{"text":"Dies ist ein Testdokument √ºber Geb√§udeversicherungen. Die Versicherungssumme betr√§gt 980000 CHF f√ºr das Hauptgeb√§ude.", "metadata":{"source":"test", "type":"insurance"}}'

# Semantische Suche nach "Versicherung Geb√§ude"
curl -X POST http://localhost:8002/v1/search \
  -H "Content-Type: application/json" \
  -d '{"query":"Versicherung Geb√§ude", "n_results":3}'

# Antwort: Dokument gefunden mit Distanz 0.72
```

## Roadmap

- [x] Phase 1: Basis-Infrastruktur (docker-compose, toolserver, chroma, llama) - **PR1**
- [ ] Phase 2: ASR/TTS Services - PR2
- [ ] Phase 3: Orchestrator mit LLM-Integration - PR3
- [ ] Phase 4: Ingestion-Pipeline - PR4
- [ ] Phase 5: Proaktiv-Engine - PR5

## Sicherheit

- Alle Services sind nur auf localhost verf√ºgbar
- Sensible Daten werden in `.env` gespeichert (nicht in Git)
- Docker-Container laufen mit minimalen Berechtigungen
- Verschl√ºsselte Volumes f√ºr sensible Daten

## Autor

Mario Hunziker (@b4llu97)

---

**Link zur Devin Session**: https://app.devin.ai/sessions/299414aacd4143aab9f366e2a23f8d7e
